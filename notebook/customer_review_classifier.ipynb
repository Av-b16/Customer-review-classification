{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a11f45-c77e-40dc-afbb-8cdc84c0833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../preprocessing/tfidf_vectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     59\u001b[39m X = tfidf_vectorizer.fit_transform(data[\u001b[33m'\u001b[39m\u001b[33mcleaned_review\u001b[39m\u001b[33m'\u001b[39m]).toarray()\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Save TF-IDF model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../preprocessing/tfidf_vectorizer.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     63\u001b[39m     pickle.dump(tfidf_vectorizer, file)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Target Variable\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../preprocessing/tfidf_vectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Load Dataset\n",
    "# ------------------------------------------\n",
    "# Load IMDb Dataset\n",
    "data = pd.read_csv(\"../data/IMDB Dataset.csv\")\n",
    "\n",
    "# View sample data\n",
    "data.head()\n",
    "\n",
    "# Check for missing values\n",
    "data.isnull().sum()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Data Preprocessing\n",
    "# ------------------------------------------\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply text cleaning\n",
    "data['cleaned_review'] = data['review'].apply(clean_text)\n",
    "\n",
    "# Convert sentiment labels to binary\n",
    "data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Feature Extraction using TF-IDF\n",
    "# ------------------------------------------\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X = tfidf_vectorizer.fit_transform(data['cleaned_review']).toarray()\n",
    "\n",
    "# Save TF-IDF model\n",
    "with open('../preprocessing/tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "# Target Variable\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split Data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Model Training and Evaluation\n",
    "# ------------------------------------------\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    # Train Model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Model Evaluation\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save the model\n",
    "    with open(f'../models/{model_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 1. Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "train_and_evaluate_model(logistic_model, \"logistic_model\")\n",
    "\n",
    "# 2. Naive Bayes\n",
    "naive_bayes_model = MultinomialNB()\n",
    "train_and_evaluate_model(naive_bayes_model, \"naive_bayes_model\")\n",
    "\n",
    "# 3. Support Vector Machine (SVM)\n",
    "svm_model = SVC(kernel='linear')\n",
    "train_and_evaluate_model(svm_model, \"svm_model\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Model Comparison\n",
    "# ------------------------------------------\n",
    "models = [\"Logistic Regression\", \"Naive Bayes\", \"SVM\"]\n",
    "accuracies = [accuracy_score(y_test, logistic_model.predict(X_test)),\n",
    "              accuracy_score(y_test, naive_bayes_model.predict(X_test)),\n",
    "              accuracy_score(y_test, svm_model.predict(X_test))]\n",
    "\n",
    "# Plot Model Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=models, y=accuracies, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd45858-955a-4f01-a00f-87ad568710c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clean_text\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_features\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_training\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_and_save_model, evaluate_model\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.preprocessing.text_preprocessing import clean_text\n",
    "from src.extraction.feature_extraction import extract_features\n",
    "from src.model_training.model_training import train_and_save_model, evaluate_model\n",
    "from src.utils.model_utils import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "data_path = \"../data/IMDB Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Check Data\n",
    "df.head()\n",
    "\n",
    "# Data Preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Split Data\n",
    "X = df['cleaned_review']\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Extraction\n",
    "X_train_tfidf, X_test_tfidf, vectorizer = extract_features(X_train, X_test)\n",
    "\n",
    "# Model Paths\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model_path = os.path.join(models_dir, \"logistic_regression.pkl\")\n",
    "train_and_save_model(logistic_model, X_train_tfidf, y_train, logistic_model_path)\n",
    "logistic_model_loaded = load_model(logistic_model_path)\n",
    "print(\"Logistic Regression Results:\")\n",
    "evaluate_model(logistic_model_loaded, X_test_tfidf, y_test)\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model_path = os.path.join(models_dir, \"naive_bayes.pkl\")\n",
    "train_and_save_model(nb_model, X_train_tfidf, y_train, nb_model_path)\n",
    "nb_model_loaded = load_model(nb_model_path)\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "evaluate_model(nb_model_loaded, X_test_tfidf, y_test)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model_path = os.path.join(models_dir, \"svm_model.pkl\")\n",
    "train_and_save_model(svm_model, X_train_tfidf, y_train, svm_model_path)\n",
    "svm_model_loaded = load_model(svm_model_path)\n",
    "print(\"\\nSupport Vector Machine Results:\")\n",
    "evaluate_model(svm_model_loaded, X_test_tfidf, y_test)\n",
    "\n",
    "# Test on a sample review\n",
    "def predict_review(review, model_path, vectorizer):\n",
    "    cleaned_review = clean_text(review)\n",
    "    vectorized_review = vectorizer.transform([cleaned_review])\n",
    "    model = load_model(model_path)\n",
    "    prediction = model.predict(vectorized_review)[0]\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Sample Prediction\n",
    "sample_review = \"The movie was absolutely fantastic and I loved every part of it!\"\n",
    "result = predict_review(sample_review, logistic_model_path, vectorizer)\n",
    "print(f\"\\nSample Review Prediction (Logistic Regression): {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61123061-0dc0-4b14-ad47-e76759b60df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clean_text\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_features\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_training\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_and_save_model, evaluate_model\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.data_preprocessing import clean_text\n",
    "from src.feature_extraction import extract_features\n",
    "from src.model_training import train_and_save_model, evaluate_model\n",
    "from src.utils import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "data_path = \"../data/IMDB Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Check Data\n",
    "df.head()\n",
    "\n",
    "# Data Preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Split Data\n",
    "X = df['cleaned_review']\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Extraction\n",
    "X_train_tfidf, X_test_tfidf, vectorizer = extract_features(X_train, X_test)\n",
    "\n",
    "# Model Paths\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model_path = os.path.join(models_dir, \"logistic_regression.pkl\")\n",
    "train_and_save_model(logistic_model, X_train_tfidf, y_train, logistic_model_path)\n",
    "logistic_model_loaded = load_model(logistic_model_path)\n",
    "print(\"Logistic Regression Results:\")\n",
    "evaluate_model(logistic_model_loaded, X_test_tfidf, y_test)\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model_path = os.path.join(models_dir, \"naive_bayes.pkl\")\n",
    "train_and_save_model(nb_model, X_train_tfidf, y_train, nb_model_path)\n",
    "nb_model_loaded = load_model(nb_model_path)\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "evaluate_model(nb_model_loaded, X_test_tfidf, y_test)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model_path = os.path.join(models_dir, \"svm_model.pkl\")\n",
    "train_and_save_model(svm_model, X_train_tfidf, y_train, svm_model_path)\n",
    "svm_model_loaded = load_model(svm_model_path)\n",
    "print(\"\\nSupport Vector Machine Results:\")\n",
    "evaluate_model(svm_model_loaded, X_test_tfidf, y_test)\n",
    "\n",
    "# Test on a sample review\n",
    "def predict_review(review, model_path, vectorizer):\n",
    "    cleaned_review = clean_text(review)\n",
    "    vectorized_review = vectorizer.transform([cleaned_review])\n",
    "    model = load_model(model_path)\n",
    "    prediction = model.predict(vectorized_review)[0]\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Sample Prediction\n",
    "sample_review = \"The movie was absolutely fantastic and I loved every part of it!\"\n",
    "result = predict_review(sample_review, logistic_model_path, vectorizer)\n",
    "print(f\"\\nSample Review Prediction (Logistic Regression): {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961511a8-17c1-4fd7-9ccd-700da6a6971e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
